"""
AI Message Handler
X·ª≠ l√Ω messages t·ª´ RabbitMQ v√† route t·ªõi handlers t∆∞∆°ng ·ª©ng
"""
import logging
import time
import json
from datetime import datetime
from typing import Dict, Any, List
import os

from app.config.settings import settings

# Gemini API
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logging.warning("‚ö†Ô∏è Gemini SDK not installed. Install: pip install google-generativeai")

# Local AI Model imports
try:
    from transformers import (
        AutoTokenizer, 
        AutoModelForSeq2SeqLM,
        BartForConditionalGeneration,
        AutoModelForCausalLM
    )
    import torch
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False
    logging.warning("‚ö†Ô∏è AI libraries not installed. Running in MOCK mode.")

logger = logging.getLogger(__name__)


class AIMessageHandler:
    """Handler ch√≠nh cho AI messages"""
    
    def __init__(self, rabbitmq_manager=None):
        """
        Initialize handler with Gemini API or local AI model
        """
        self.mock_mode = settings.MOCK_MODE
        self.ai_provider = settings.AI_PROVIDER
        self.gemini_client = None
        self.model = None
        self.tokenizer = None
        self.device = None
        self.rabbitmq_manager = rabbitmq_manager
        
        # Initialize AI based on provider
        if not self.mock_mode:
            if self.ai_provider == 'gemini' and GEMINI_AVAILABLE:
                try:
                    self._init_gemini()
                except Exception as e:
                    logger.error(f"‚ùå Failed to init Gemini: {e}")
                    logger.warning("‚ö†Ô∏è Falling back to local model or MOCK mode")
                    if AI_AVAILABLE:
                        try:
                            self._load_summarize_model()
                        except Exception as e2:
                            logger.error(f"‚ùå Local model also failed: {e2}")
                            self.mock_mode = True
                    else:
                        self.mock_mode = True
            elif AI_AVAILABLE:
                try:
                    self._load_summarize_model()
                except Exception as e:
                    logger.error(f"‚ùå Failed to load AI model: {e}")
                    logger.warning("‚ö†Ô∏è Falling back to MOCK mode")
                    self.mock_mode = True
            else:
                logger.warning("‚ö†Ô∏è No AI available, using MOCK mode")
                self.mock_mode = True
        
        mode = "MOCK" if self.mock_mode else f"{self.ai_provider.upper()} AI"
        logger.info(f"ü§ñ AI Message Handler initialized in {mode} mode")
    
    def _init_gemini(self):
        """Initialize Gemini API client"""
        api_key = settings.GEMINI_API_KEY
        if not api_key or api_key == 'your_api_key_here':
            raise ValueError("GEMINI_API_KEY not configured")
        
        genai.configure(api_key=api_key)
        model_name = settings.GEMINI_MODEL
        self.gemini_client = genai.GenerativeModel(model_name)
        
        logger.info(f"‚úÖ Gemini initialized: {model_name}")
        logger.info(f"üìä Free tier: 1500 req/day, 1M tokens/day")
    
    def handle_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        Route message t·ªõi handler ph√π h·ª£p d·ª±a tr√™n action
        
        Args:
            message: Message dict t·ª´ RabbitMQ (format: AIMessageRequest)
            
        Returns:
            Response dict v·ªõi status v√† result (format: AIMessageResponse)
        """
        action = message.get('action')
        message_id = message.get('messageId') or message.get('message_id')  # Support both formats
        payload = message.get('payload', {})
        priority = message.get('priority', 'MEDIUM')
        user_id = message.get('userId') or message.get('user_id')
        
        start_time = datetime.now()
        
        try:
            logger.info(f"[Received] Action: {action} for Message ID: {message_id}")
            logger.info(f"[Priority] {priority} | User: {user_id}")
            mode_status = "MOCK mode" if self.mock_mode else "AI mode"
            logger.info(f"[Processing] {action} with {mode_status}...")
            
            # Route to appropriate handler
            if action == 'MAP_CLO_PLO':
                result = self._handle_map_clo_plo(message_id, payload)
            elif action == 'COMPARE_VERSIONS':
                result = self._handle_compare_versions(message_id, payload)
            elif action == 'SUMMARIZE_SYLLABUS':
                result = self._handle_summarize(message_id, payload)
            else:
                raise ValueError(f"Unknown action: {action}")
            
            processing_time = int((datetime.now() - start_time).total_seconds() * 1000)
            
            response = {
                'messageId': message_id,
                'action': action,
                'status': 'SUCCESS',
                'progress': 100,
                'result': result,
                'processingTimeMs': processing_time
            }
            
            logger.info(f"[Done] Processing completed.")
            logger.info(f"‚úÖ {action} completed in {processing_time}ms")
            
            # ‚úÖ Save result to database (Transactional Outbox pattern)
            await self._save_to_database(message_id, action, result, processing_time, payload)
            
            # Send result to result queue
            self._send_result_to_queue(response)
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Error handling {action}: {e}", exc_info=True)
            
            processing_time = int((datetime.now() - start_time).total_seconds() * 1000)
            
            # Error response
            error_response = {
                'messageId': message_id,
                'action': action,
                'status': 'ERROR',
                'progress': 0,
                'result': None,
                'errorMessage': str(e),
                'processingTimeMs': processing_time
            }
            
            # Send error to result queue
            self._send_result_to_queue(error_response)
            
            return error_response
    
    def _send_result_to_queue(self, response: Dict[str, Any]) -> None:
        """Send result to ai_result_queue"""
        if not self.rabbitmq_manager:
            logger.warning("‚ö†Ô∏è No RabbitMQ manager, skipping result publish")
            return
        
        try:
            result_queue = os.getenv('QUEUE_AI_RESULT', 'ai_result_queue')
            success = self.rabbitmq_manager.publish_message(result_queue, response)
            if success:
                logger.info(f"üì§ Result sent to {result_queue}: {response.get('messageId')}")
            else:
                logger.error(f"‚ùå Failed to send result to {result_queue}")
        except Exception as e:
            logger.error(f"‚ùå Error sending result: {e}", exc_info=True)
    
    async def _save_to_database(
        self,
        message_id: str,
        action: str,
        result: Dict[str, Any],
        processing_time: int,
        payload: Dict[str, Any]
    ) -> None:
        """
        Save analysis result to database
        
        ‚úÖ IMPLEMENTED: Database persistence for audit and history
        
        This implements the "Database per Service" pattern:
        - AI Service owns ai_service schema
        - Core Service queries via API (not direct DB access)
        
        Args:
            message_id: Task ID
            action: Analysis type
            result: Analysis result
            processing_time: Processing time in ms
            payload: Original request payload
        """
        try:
            from app.database.repository import AnalysisRepository
            
            # Extract syllabus_version_id from payload
            syllabus_id = payload.get('syllabus_id')
            if not syllabus_id:
                logger.warning(f"‚ö†Ô∏è No syllabus_id in payload, skipping database save")
                return
            
            # Determine model used
            model_used = "gemini-pro" if self.ai_provider == 'gemini' else "mock"
            if self.mock_mode:
                model_used = "mock"
            
            # Calculate confidence score (mock for now)
            confidence_score = 0.85  # TODO: Get from AI model
            
            # Save to database
            success = await AnalysisRepository.save_analysis(
                analysis_id=message_id,
                syllabus_version_id=syllabus_id,
                analysis_type=action,
                result=result,
                model_used=model_used,
                confidence_score=confidence_score,
                processing_time_ms=processing_time
            )
            
            if success:
                logger.info(f"üíæ Saved to database: {message_id}")
            else:
                logger.error(f"‚ùå Failed to save to database: {message_id}")
                
        except Exception as e:
            logger.error(f"‚ùå Error saving to database: {e}", exc_info=True)
            # Don't raise - database save failure shouldn't block RabbitMQ result
    
    def _handle_map_clo_plo(self, message_id: str, payload: Dict) -> Dict:
        """
        Handler cho MAP_CLO_PLO - Ki·ªÉm tra tu√¢n th·ªß CLO-PLO
        
        MOCK DATA - Tr·∫£ v·ªÅ structure gi·ªëng th·∫≠t ƒë·ªÉ test workflow
        """
        syllabus_id = payload.get('syllabus_id')
        curriculum_id = payload.get('curriculum_id')
        
        logger.info(f"üìä Analyzing CLO-PLO mapping for syllabus: {syllabus_id}")
        
        # Simulate AI processing time
        time.sleep(2)  # 2 seconds
        
        # MOCK RESULT - ƒê√∫ng format theo k·∫ø ho·∫°ch
        result = {
            "overall_status": "NEEDS_IMPROVEMENT",
            "compliance_score": 75.5,
            "issues": [
                {
                    "severity": "HIGH",
                    "type": "MISSING_PLO_MAPPING",
                    "code": "PLO2",
                    "title": "PLO2: CLO ch∆∞a √°nh x·∫° ƒë·ªß sang PLO2 (y√™u c·∫ßu t·ªëi thi·ªÉu 2 CLO)",
                    "description": "Hi·ªán t·∫°i ch·ªâ c√≥ 1 CLO √°nh x·∫° sang PLO2, c·∫ßn th√™m √≠t nh·∫•t 1 CLO n·ªØa",
                    "current_count": 1,
                    "required_count": 2,
                    "affected_clos": ["CLO-1"]
                },
                {
                    "severity": "MEDIUM",
                    "type": "INSUFFICIENT_WEIGHT",
                    "code": "PLO5",
                    "title": "PLO5: Thi·∫øu ƒë√°nh gi√° k·ªπ nƒÉng l√†m vi·ªác nh√≥m cho PLO5",
                    "description": "PLO5 y√™u c·∫ßu k·ªπ nƒÉng l√†m vi·ªác nh√≥m nh∆∞ng ch·ªâ c√≥ 5% tr·ªçng s·ªë trong ƒë√°nh gi√° (khuy·∫øn ngh·ªã 10-15%)",
                    "current_weight": 5,
                    "recommended_weight": "10-15",
                    "affected_assessments": ["B√†i t·∫≠p nh√≥m"]
                }
            ],
            "suggestions": [
                {
                    "priority": 1,
                    "action": "ADD_CLO",
                    "title": "Th√™m CLO v·ªÅ k·ªπ nƒÉng ph√¢n t√≠ch d·ªØ li·ªáu ·ª©ng PLO2",
                    "description": "V√≠ d·ª•: 'Sinh vi√™n c√≥ kh·∫£ nƒÉng ph√¢n t√≠ch y√™u c·∫ßu v√† thi·∫øt k·∫ø m√¥ h√¨nh d·ªØ li·ªáu ph√π h·ª£p'"
                },
                {
                    "priority": 2,
                    "action": "ADJUST_WEIGHT",
                    "title": "B·ªï sung ph∆∞∆°ng ph√°p ƒë√°nh gi√° nh√≥m (weight 10-15%) cho PLO5",
                    "description": "TƒÉng tr·ªçng s·ªë b√†i t·∫≠p nh√≥m t·ª´ 5% l√™n 15%"
                },
                {
                    "priority": 3,
                    "action": "REVIEW_CONSISTENCY",
                    "title": "Xem x√©t tƒÉng tr·ªçng s·ªë CLO √°nh x·∫° sang PLO2 l√™n √≠t nh·∫•t 30%",
                    "description": "ƒê·∫£m b·∫£o t·∫ßm quan tr·ªçng c·ªßa PLO2 ƒë∆∞·ª£c ph·∫£n √°nh qua assessment scheme"
                }
            ],
            "compliant_mappings": [
                {
                    "plo_code": "PLO1",
                    "mapped_clos": ["CLO-1", "CLO-2", "CLO-3"],
                    "total_weight": 45,
                    "status": "GOOD"
                },
                {
                    "plo_code": "PLO3",
                    "mapped_clos": ["CLO-4", "CLO-5"],
                    "total_weight": 35,
                    "status": "GOOD"
                }
            ]
        }
        
        logger.info(f" CLO-PLO analysis completed. Status: {result['overall_status']}")
        return result
    
    def _handle_compare_versions(self, message_id: str, payload: Dict) -> Dict:
        """
        Handler cho COMPARE_VERSIONS - So s√°nh phi√™n b·∫£n
        
        REAL IMPLEMENTATION with Gemini AI
        """
        old_version_id = payload.get('old_version_id')
        new_version_id = payload.get('new_version_id')
        old_version = payload.get('old_version', {})
        new_version = payload.get('new_version', {})
        
        logger.info(f"üîç Comparing versions: {old_version.get('version_no')} ‚Üí {new_version.get('version_no')}")
        logger.info(f" Old version: ID={old_version_id[:8]}..., version_no={old_version.get('version_no')}, CLOs={len(old_version.get('content', {}).get('clos', []))}")
        logger.info(f" New version: ID={new_version_id[:8]}..., version_no={new_version.get('version_no')}, CLOs={len(new_version.get('content', {}).get('clos', []))}")
        
        # Extract content from both versions
        old_content = old_version.get('content', {})
        new_content = new_version.get('content', {})
        
        logger.info(f" Comparing all sections of the syllabus")
        
        # Detect changes
        changes = []
        sections_affected = []
        
        # 1. Compare CLOs
        old_clos = old_content.get('clos', [])
        new_clos = new_content.get('clos', [])
        if old_clos != new_clos:
            sections_affected.append("learning_outcomes")
            clo_changes = self._compare_clos(old_clos, new_clos)
            if clo_changes:
                changes.append({
                    "section": "learning_outcomes",
                    "section_title": "M·ª•c ti√™u h·ªçc t·∫≠p (CLOs)",
                    "change_type": "MODIFIED",
                    "changes": clo_changes
                })
        
        # 2. Compare Assessment Schemes
        old_assessments = old_content.get('assessment_schemes', [])
        new_assessments = new_content.get('assessment_schemes', [])
        if old_assessments != new_assessments:
            sections_affected.append("assessment_schemes")
            assessment_changes = self._compare_assessments(old_assessments, new_assessments)
            if assessment_changes:
                changes.append({
                    "section": "assessment_schemes",
                    "section_title": "Ph∆∞∆°ng ph√°p ƒë√°nh gi√°",
                    "change_type": "MODIFIED",
                    "changes": assessment_changes
                })
        
        # 3. Compare Teaching Methods
        old_methods = old_content.get('teaching_methods', [])
        new_methods = new_content.get('teaching_methods', [])
        if old_methods != new_methods:
            sections_affected.append("teaching_methods")
            method_changes = self._compare_teaching_methods(old_methods, new_methods)
            if method_changes:
                changes.append({
                    "section": "teaching_methods",
                    "section_title": "Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y",
                    "change_type": "MODIFIED",
                    "changes": method_changes
                })
        
        # 4. Compare Prerequisites
        old_prereqs = old_content.get('prerequisites', [])
        new_prereqs = new_content.get('prerequisites', [])
        if old_prereqs != new_prereqs:
            sections_affected.append("prerequisites")
            prereq_changes = self._compare_prerequisites(old_prereqs, new_prereqs)
            if prereq_changes:
                changes.append({
                    "section": "prerequisites",
                    "section_title": "M√¥n h·ªçc ti√™n quy·∫øt",
                    "change_type": "MODIFIED",
                    "changes": prereq_changes
                })
        
        # 5. Compare Learning Materials
        old_materials = old_content.get('learning_materials', [])
        new_materials = new_content.get('learning_materials', [])
        if old_materials != new_materials:
            sections_affected.append("learning_materials")
            material_changes = self._compare_learning_materials(old_materials, new_materials)
            if material_changes:
                changes.append({
                    "section": "learning_materials",
                    "section_title": "T√†i li·ªáu h·ªçc t·∫≠p",
                    "change_type": "MODIFIED",
                    "changes": material_changes
                })
        
        # 6. Compare Weekly Plans
        old_weekly = old_content.get('weekly_plans', [])
        new_weekly = new_content.get('weekly_plans', [])
        if old_weekly != new_weekly:
            sections_affected.append("weekly_plans")
            weekly_changes = self._compare_weekly_plans(old_weekly, new_weekly)
            if weekly_changes:
                changes.append({
                    "section": "weekly_plans",
                    "section_title": "K·∫ø ho·∫°ch gi·∫£ng d·∫°y theo tu·∫ßn",
                    "change_type": "MODIFIED",
                    "changes": weekly_changes
                })
        
        # 7. Compare Description
        if old_version.get('description') != new_version.get('description'):
            sections_affected.append("description")
            changes.append({
                "section": "description",
                "section_title": "M√¥ t·∫£ m√¥n h·ªçc",
                "change_type": "MODIFIED",
                "changes": [{
                    "field": "N·ªôi dung m√¥ t·∫£",
                    "old_value": old_version.get('description', ''),
                    "new_value": new_version.get('description', ''),
                    "significance": "MEDIUM"
                }]
            })
        
        # 8. Compare Objectives
        if old_version.get('objectives') != new_version.get('objectives'):
            sections_affected.append("objectives")
            changes.append({
                "section": "objectives",
                "section_title": "M·ª•c ti√™u m√¥n h·ªçc",
                "change_type": "MODIFIED",
                "changes": [{
                    "field": "N·ªôi dung m·ª•c ti√™u",
                    "old_value": old_version.get('objectives', ''),
                    "new_value": new_version.get('objectives', ''),
                    "significance": "HIGH"
                }]
            })
        
        # 9. Compare Credit Count
        if old_version.get('credit_count') != new_version.get('credit_count'):
            sections_affected.append("credit_count")
            changes.append({
                "section": "credit_count",
                "section_title": "S·ªë t√≠n ch·ªâ",
                "change_type": "MODIFIED",
                "changes": [{
                    "field": "S·ªë t√≠n ch·ªâ",
                    "old_value": str(old_version.get('credit_count', '')),
                    "new_value": str(new_version.get('credit_count', '')),
                    "significance": "HIGH"
                }]
            })
        
        # Count change types
        total_changes = len(changes)
        major_changes = sum(1 for c in changes if any(ch.get('significance') == 'HIGH' for ch in c.get('changes', [])))
        minor_changes = total_changes - major_changes
        
        # AI Analysis with Gemini
        ai_analysis = self._get_ai_comparison_analysis(old_version, new_version, changes) if self.gemini_client and total_changes > 0 else None
        
        result = {
            "is_first_version": False,
            "version_history": [
                {
                    "version_number": new_version.get('version_number', 1),
                    "version_no": new_version.get('version_no', 'v1'),
                    "status": "Hi·ªán t·∫°i",
                    "created_at": new_version.get('created_at', ''),
                    "is_current": True
                },
                {
                    "version_number": old_version.get('version_number', 1),
                    "version_no": old_version.get('version_no', 'v1'),
                    "status": old_version.get('status', 'REJECTED'),
                    "created_at": old_version.get('created_at', ''),
                    "is_current": False
                }
            ],
            "changes_summary": {
                "total_changes": total_changes,
                "major_changes": major_changes,
                "minor_changes": minor_changes,
                "sections_affected": sections_affected
            },
            "detailed_changes": changes,
            "ai_analysis": ai_analysis
        }
        
        logger.info(f" Version comparison completed: {total_changes} changes detected")
        return result
    
    def _compare_clos(self, old_clos: List, new_clos: List) -> List[Dict]:
        """So s√°nh CLOs gi·ªØa 2 versions"""
        changes = []
        
        # Build maps by code
        old_map = {clo.get('code'): clo for clo in old_clos}
        new_map = {clo.get('code'): clo for clo in new_clos}
        
        # Find added CLOs
        for code in new_map:
            if code not in old_map:
                changes.append({
                    "field": f"CLO {code}",
                    "old_value": None,
                    "new_value": new_map[code].get('description'),
                    "significance": "HIGH",
                    "impact": "Th√™m m·ªõi CLO"
                })
        
        # Find removed CLOs
        for code in old_map:
            if code not in new_map:
                changes.append({
                    "field": f"CLO {code}",
                    "old_value": old_map[code].get('description'),
                    "new_value": None,
                    "significance": "HIGH",
                    "impact": "X√≥a CLO"
                })
        
        # Find modified CLOs
        for code in old_map:
            if code in new_map:
                old_clo = old_map[code]
                new_clo = new_map[code]
                if old_clo.get('description') != new_clo.get('description'):
                    changes.append({
                        "field": f"CLO {code}",
                        "old_value": old_clo.get('description'),
                        "new_value": new_clo.get('description'),
                        "significance": "HIGH",
                        "impact": "Thay ƒë·ªïi n·ªôi dung CLO"
                    })
        
        return changes
    
    def _compare_assessments(self, old_assessments: List, new_assessments: List) -> List[Dict]:
        """So s√°nh assessment schemes"""
        changes = []
        
        # Build maps by type
        old_map = {a.get('assessment_type'): a for a in old_assessments}
        new_map = {a.get('assessment_type'): a for a in new_assessments}
        
        # Find added assessments
        for atype in new_map:
            if atype not in old_map:
                changes.append({
                    "field": f"ƒê√°nh gi√° {atype}",
                    "old_value": None,
                    "new_value": f"{new_map[atype].get('weight_percentage', 0)}%",
                    "significance": "HIGH",
                    "impact": "Th√™m m·ªõi ph∆∞∆°ng ph√°p ƒë√°nh gi√°"
                })
        
        # Find removed assessments
        for atype in old_map:
            if atype not in new_map:
                changes.append({
                    "field": f"ƒê√°nh gi√° {atype}",
                    "old_value": f"{old_map[atype].get('weight_percentage', 0)}%",
                    "new_value": None,
                    "significance": "HIGH",
                    "impact": "X√≥a ph∆∞∆°ng ph√°p ƒë√°nh gi√°"
                })
        
        # Find modified assessments
        for atype in old_map:
            if atype in new_map:
                old_weight = old_map[atype].get('weight_percentage', 0)
                new_weight = new_map[atype].get('weight_percentage', 0)
                if old_weight != new_weight:
                    changes.append({
                        "field": f"ƒê√°nh gi√° {atype}",
                        "old_value": f"{old_weight}%",
                        "new_value": f"{new_weight}%",
                        "significance": "HIGH",
                        "impact": "Thay ƒë·ªïi t·ª∑ tr·ªçng ƒë√°nh gi√°"
                    })
        
        return changes
    
    def _compare_teaching_methods(self, old_methods: List, new_methods: List) -> List[Dict]:
        """So s√°nh teaching methods"""
        changes = []
        
        old_names = set(m.get('method_name', '') for m in old_methods)
        new_names = set(m.get('method_name', '') for m in new_methods)
        
        # Find added methods
        for name in new_names - old_names:
            changes.append({
                "field": "Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y",
                "old_value": None,
                "new_value": name,
                "significance": "MEDIUM",
                "impact": "Th√™m ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y m·ªõi"
            })
        
        # Find removed methods
        for name in old_names - new_names:
            changes.append({
                "field": "Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y",
                "old_value": name,
                "new_value": None,
                "significance": "MEDIUM",
                "impact": "X√≥a ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y"
            })
        
        return changes
    
    def _compare_prerequisites(self, old_prereqs: List, new_prereqs: List) -> List[Dict]:
        """So s√°nh prerequisites"""
        changes = []
        
        old_codes = set(p.get('subject_code', '') for p in old_prereqs)
        new_codes = set(p.get('subject_code', '') for p in new_prereqs)
        
        # Find added prerequisites
        for code in new_codes - old_codes:
            changes.append({
                "field": "M√¥n ti√™n quy·∫øt",
                "old_value": None,
                "new_value": code,
                "significance": "HIGH",
                "impact": "Th√™m m√¥n ti√™n quy·∫øt m·ªõi"
            })
        
        # Find removed prerequisites
        for code in old_codes - new_codes:
            changes.append({
                "field": "M√¥n ti√™n quy·∫øt",
                "old_value": code,
                "new_value": None,
                "significance": "HIGH",
                "impact": "X√≥a m√¥n ti√™n quy·∫øt"
            })
        
        return changes
    
    def _compare_learning_materials(self, old_materials: List, new_materials: List) -> List[Dict]:
        """So s√°nh learning materials"""
        changes = []
        
        # Build maps by title
        old_map = {m.get('title'): m for m in old_materials}
        new_map = {m.get('title'): m for m in new_materials}
        
        # Find added materials
        for title in new_map:
            if title not in old_map:
                changes.append({
                    "field": "T√†i li·ªáu h·ªçc t·∫≠p",
                    "old_value": None,
                    "new_value": title,
                    "significance": "LOW",
                    "impact": "Th√™m t√†i li·ªáu m·ªõi"
                })
        
        # Find removed materials
        for title in old_map:
            if title not in new_map:
                changes.append({
                    "field": "T√†i li·ªáu h·ªçc t·∫≠p",
                    "old_value": title,
                    "new_value": None,
                    "significance": "LOW",
                    "impact": "X√≥a t√†i li·ªáu"
                })
        
        return changes
    
    def _compare_weekly_plans(self, old_weekly: List, new_weekly: List) -> List[Dict]:
        """So s√°nh weekly plans"""
        changes = []
        
        if len(old_weekly) != len(new_weekly):
            changes.append({
                "field": "S·ªë tu·∫ßn h·ªçc",
                "old_value": f"{len(old_weekly)} tu·∫ßn",
                "new_value": f"{len(new_weekly)} tu·∫ßn",
                "significance": "HIGH",
                "impact": "Thay ƒë·ªïi s·ªë tu·∫ßn h·ªçc"
            })
        
        # Compare week by week
        for i, (old_week, new_week) in enumerate(zip(old_weekly, new_weekly), 1):
            old_topic = old_week.get('topic', '')
            new_topic = new_week.get('topic', '')
            if old_topic != new_topic:
                changes.append({
                    "field": f"Tu·∫ßn {i}",
                    "old_value": old_topic,
                    "new_value": new_topic,
                    "significance": "MEDIUM",
                    "impact": f"Thay ƒë·ªïi n·ªôi dung tu·∫ßn {i}"
                })
        
        return changes
    
    def _get_ai_comparison_analysis(self, old_version: Dict, new_version: Dict, changes: List[Dict]) -> Dict:
        """G·ªçi Gemini AI ƒë·ªÉ ph√¢n t√≠ch s·ª± kh√°c bi·ªát"""
        if not self.gemini_client:
            return None
        
        try:
            # Build detailed change list for prompt
            change_details = []
            for change in changes[:15]:  # Limit to 15 most important changes
                section = change.get('section', 'unknown')
                section_names = {
                    'learning_outcomes': 'CLOs (Chu·∫©n ƒë·∫ßu ra)',
                    'assessment_schemes': 'Ph∆∞∆°ng ph√°p ƒë√°nh gi√°',
                    'teaching_methods': 'Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y',
                    'prerequisites': 'ƒêi·ªÅu ki·ªán ti√™n quy·∫øt',
                    'learning_materials': 'T√†i li·ªáu h·ªçc t·∫≠p',
                    'weekly_plans': 'K·∫ø ho·∫°ch gi·∫£ng d·∫°y',
                    'description': 'M√¥ t·∫£ m√¥n h·ªçc',
                    'objectives': 'M·ª•c ti√™u m√¥n h·ªçc'
                }
                section_vn = section_names.get(section, section)
                
                for detail in change.get('changes', []):
                    field = detail.get('field', '')
                    old_val = detail.get('old_value')
                    new_val = detail.get('new_value')
                    impact = detail.get('impact', '')
                    
                    if old_val is None:
                        change_details.append(f"‚Ä¢ {section_vn} - {field}: TH√äM M·ªöI '{new_val}' ({impact})")
                    elif new_val is None:
                        change_details.append(f"‚Ä¢ {section_vn} - {field}: X√ìA '{old_val}' ({impact})")
                    else:
                        change_details.append(f"‚Ä¢ {section_vn} - {field}: S·ª¨A ƒê·ªîI t·ª´ '{old_val}' ‚Üí '{new_val}' ({impact})")
            
            change_list = '\n'.join(change_details) if change_details else "Kh√¥ng c√≥ thay ƒë·ªïi ƒë√°ng k·ªÉ"
            
            prompt = f"""
Ph√¢n t√≠ch chi ti·∫øt s·ª± thay ƒë·ªïi gi·ªØa 2 phi√™n b·∫£n ƒë·ªÅ c∆∞∆°ng m√¥n h·ªçc:

**PHI√äN B·∫¢N C≈® (v{old_version.get('version_no')}):**
- S·ªë CLOs: {len(old_version.get('content', {}).get('clos', []))}
- S·ªë ph∆∞∆°ng ph√°p ƒë√°nh gi√°: {len(old_version.get('content', {}).get('assessment_schemes', []))}
- M√¥ t·∫£: {old_version.get('description', '')[:150]}...

**PHI√äN B·∫¢N M·ªöI (v{new_version.get('version_no')}):**
- S·ªë CLOs: {len(new_version.get('content', {}).get('clos', []))}
- S·ªë ph∆∞∆°ng ph√°p ƒë√°nh gi√°: {len(new_version.get('content', {}).get('assessment_schemes', []))}
- M√¥ t·∫£: {new_version.get('description', '')[:150]}...

**CHI TI·∫æT C√ÅC THAY ƒê·ªîI ({len(changes)} thay ƒë·ªïi):**
{change_list}

**Y√äU C·∫¶U PH√ÇN T√çCH:**
1. T·ªïng quan: ƒê√°nh gi√° chung v·ªÅ m·ª©c ƒë·ªô v√† t√≠nh ch·∫•t thay ƒë·ªïi (2-3 c√¢u ng·∫Øn)
2. C·∫£i ti·∫øn ch√≠nh: Li·ªát k√™ 3-4 thay ƒë·ªïi QUAN TR·ªåNG NH·∫§T theo format:
   - [M·ª•c]: N·ªôi dung thay ƒë·ªïi c·ª• th·ªÉ
   V√≠ d·ª•: "CLOs: Th√™m CLO3 v·ªÅ k·ªπ nƒÉng ph√¢n t√≠ch d·ªØ li·ªáu"
3. Khuy·∫øn ngh·ªã: 1-2 g·ª£i √Ω c·∫£i thi·ªán ti·∫øp (n·∫øu c·∫ßn)

Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, T·∫¨P TRUNG V√ÄO ƒêI·ªÇM KH√ÅC BI·ªÜT QUAN TR·ªåNG, b·∫±ng ti·∫øng Vi·ªát.
"""
            
            response = self.gemini_client.generate_content(prompt)
            analysis_text = response.text
            
            # Parse response - clean markdown formatting
            lines = [l.strip() for l in analysis_text.split('\n') if l.strip()]
            
            # Remove markdown formatting (**, ##, ---, bullets)
            clean_lines = []
            for line in lines:
                # Skip separator lines
                if line in ['---', '***', '___']:
                    continue
                # Remove markdown bold/italic
                line = line.replace('**', '').replace('__', '').replace('*', '').replace('_', '')
                # Remove heading markers
                line = line.lstrip('#').strip()
                # Remove bullet points and numbering
                if line.startswith(('- ', '+ ', '* ')):
                    line = line[2:].strip()
                elif len(line) > 2 and line[0].isdigit() and line[1:3] in ['. ', ') ']:
                    line = line[3:].strip()
                
                if line:  # Only add non-empty lines
                    clean_lines.append(line)
            
            # Categorize lines into sections
            overall = ""
            improvements = []
            recommendations = []
            current_section = "overall"
            
            for line in clean_lines:
                line_lower = line.lower()
                if any(keyword in line_lower for keyword in ['c·∫£i ti·∫øn', 'c·∫£i thi·ªán', 'improvement']):
                    current_section = "improvements"
                    continue
                elif any(keyword in line_lower for keyword in ['khuy·∫øn ngh·ªã', 'ƒë·ªÅ xu·∫•t', 'recommendation']):
                    current_section = "recommendations"
                    continue
                
                # Add to appropriate section
                if current_section == "overall" and not overall:
                    overall = line
                elif current_section == "improvements":
                    improvements.append(line)
                elif current_section == "recommendations":
                    recommendations.append(line)
            
            return {
                "overall_assessment": overall if overall else (clean_lines[0] if clean_lines else "Phi√™n b·∫£n m·ªõi c√≥ c·∫£i thi·ªán so v·ªõi phi√™n b·∫£n c≈©"),
                "key_improvements": improvements if improvements else clean_lines[1:4],
                "recommendations": recommendations if recommendations else clean_lines[4:]
            }
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get AI analysis: {e}")
            return {
                "overall_assessment": "Kh√¥ng th·ªÉ ph√¢n t√≠ch b·∫±ng AI",
                "key_improvements": [],
                "recommendations": []
            }
    
    def _handle_summarize(self, message_id: str, payload: Dict) -> Dict:
        """
        Handler cho SUMMARIZE_SYLLABUS - T√≥m t·∫Øt cho sinh vi√™n
        
        S·ª≠ d·ª•ng AI model th·∫≠t (VietAI/vit5-base ho·∫∑c Gemini) ƒë·ªÉ t√≥m t·∫Øt
        """
        syllabus_id = payload.get('syllabus_id')
        syllabus_data = payload.get('syllabus_data', {})
        
        logger.info(f"üìù Summarizing syllabus: {syllabus_id}")
        logger.info(f"üîç AI Status: mock_mode={self.mock_mode}, model_loaded={self.model is not None}, gemini_available={self.gemini_client is not None}")
        
        # Use real AI if available (Gemini or local model)
        if not self.mock_mode and (self.gemini_client is not None or self.model is not None):
            ai_provider = "GEMINI" if self.gemini_client else "LOCAL MODEL"
            logger.info(f"‚úÖ Using {ai_provider} for summarization")
            return self._summarize_with_ai(syllabus_data)
        
        # Fallback to mock
        logger.warning("‚ö†Ô∏è Using MOCK data (AI model not available or mock_mode=true)")
        time.sleep(2)  # 2 seconds
        
        # MOCK RESULT
        result = {
            "overview": {
                "title": "Thi·∫øt k·∫ø v√† t·ªëi ∆∞u h√≥a CSDL",
                "description": "M√¥n h·ªçc trang b·ªã ki·∫øn th·ª©c v·ªÅ thi·∫øt k·∫ø CSDL quan h·ªá, chu·∫©n h√≥a, v√† t·ªëi ∆∞u truy v·∫•n"
            },
            "highlights": {
                "difficulty": {
                    "level": "MEDIUM",
                    "description": "Trung b√¨nh - Ph√π h·ª£p sinh vi√™n nƒÉm 2-3"
                },
                "duration": {
                    "theory_hours": 30,
                    "practice_hours": 30,
                    "total_hours": 60,
                    "description": "30 l√Ω thuy·∫øt + 30 ti·∫øt th·ª±c h√†nh"
                },
                "assessment": {
                    "summary": "C√¢n b·∫±ng gi·ªØa thi v√† b√†i t·∫≠p/d·ª± √°n",
                    "breakdown": [
                        {"type": "Thi gi·ªØa k·ª≥", "weight": 30},
                        {"type": "B√†i t·∫≠p", "weight": 20},
                        {"type": "D·ª± √°n", "weight": 20},
                        {"type": "Thi cu·ªëi k·ª≥", "weight": 30}
                    ]
                },
                "skills_acquired": {
                    "summary": "√Ånh x·∫° CLO t·ªõi PLO r√µ r√†ng",
                    "key_skills": [
                        "Thi·∫øt k·∫ø ERD v√† chu·∫©n h√≥a CSDL",
                        "Vi·∫øt truy v·∫•n SQL ph·ª©c t·∫°p",
                        "T·ªëi ∆∞u hi·ªáu nƒÉng database"
                    ]
                }
            },
            "recommendations": {
                "prerequisites": {
                    "required": ["C·∫•u tr√∫c d·ªØ li·ªáu v√† gi·∫£i thu·∫≠t", "OOP"],
                    "description": "N√™n c√≥ ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ c√°c m√¥n ti√™n quy·∫øt"
                },
                "preparation": {
                    "tips": [
                        "√în l·∫°i ki·∫øn th·ª©c n·ªÅn v·ªÅ c·∫•u tr√∫c d·ªØ li·ªáu",
                        "L√†m quen v·ªõi SQL c∆° b·∫£n",
                        "C√†i ƒë·∫∑t PostgreSQL/MySQL tr∆∞·ªõc khi h·ªçc"
                    ],
                    "description": "Chu·∫©n b·ªã tr∆∞·ªõc: √în l·∫°i ki·∫øn th·ª©c n·ªÅn"
                },
                "study_time": {
                    "hours_per_week": 6,
                    "breakdown": "4 gi·ªù l√†m b√†i t·∫≠p + 2 gi·ªù ƒë·ªçc t√†i li·ªáu",
                    "description": "D√†nh √≠t nh·∫•t 6 gi·ªù/tu·∫ßn"
                }
            }
        }
        
        logger.info(f"‚úÖ Summarization completed")
        return result
    
    # =============================================
    # AI MODEL METHODS - REAL IMPLEMENTATION
    # =============================================
    
    def _load_summarize_model(self):
        """
        Load VietAI T5 model cho Vietnamese summarization
        Specialized model trained on Vietnamese news summarization
        """
        model_name = os.getenv('AI_MODEL_NAME', 'VietAI/vit5-large-vietnews-summarization')
        use_8bit = os.getenv('USE_8BIT_QUANTIZATION', 'false').lower() == 'true'
        
        logger.info(f"üì¶ Loading model: {model_name}")
        logger.info(f"üîß 8-bit quantization: {use_8bit}")
        
        # Determine device
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üîß Using device: {self.device}")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Load model - Seq2Seq for T5 summarization
        logger.info("‚è≥ Loading model... (first time may take 1-2 minutes to download ~1.2GB)")
        
        self.model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        ).to(self.device)
        
        logger.info(f"‚úÖ Model loaded successfully on {self.device}")
        if use_8bit:
            logger.info("‚úÖ Using 8-bit quantization (reduced memory usage)")
    
    def _summarize_text(self, text: str, max_length: int = 100) -> str:
        """
        Summarize text using Gemini API or local model
        Falls back to extractive if AI not available
        """
        if not text or not isinstance(text, str):
            return text
        
        text = ' '.join(text.split())
        
        # Only skip summarization if text is already VERY SHORT (< 60 chars)
        if len(text) <= 60:
            return text
        
        # Try Gemini first
        if self.gemini_client and self.ai_provider == 'gemini':
            try:
                return self._summarize_with_gemini(text, max_length)
            except Exception as e:
                logger.error(f"‚ùå [GEMINI FAILED] {str(e)}")
                logger.info("üìã [FALLBACK] Trying extractive method")
                return self._extractive_summarize(text, max_length)
        
        # Try local model
        if self.model and self.tokenizer:
            try:
                return self._summarize_with_local_model(text, max_length)
            except Exception as e:
                logger.error(f"‚ùå [LOCAL MODEL FAILED] {str(e)}")
                logger.info("üìã [FALLBACK] Using extractive method")
                return self._extractive_summarize(text, max_length)
        
        # Fallback to extractive
        logger.warning("üìã [FALLBACK MODE] No AI available, using extractive method")
        return self._extractive_summarize(text, max_length)
    
    def _summarize_with_gemini(self, text: str, max_length: int) -> str:
        """Summarize using Gemini API"""
        logger.info(f"ü§ñ [GEMINI] Summarizing text: {len(text)} chars ‚Üí target {max_length} chars")
        
        prompt = f"""R√∫t g·ªçn vƒÉn b·∫£n sau th√†nh T·ªêI ƒêA {max_length} k√Ω t·ª±.

Y√™u c·∫ßu:
- CH·ªà gi·ªØ T·ª™ KH√ìA CH√çNH
- B·ªé v√≠ d·ª• trong ngo·∫∑c (), chi ti·∫øt d√†i d√≤ng
- Vi·∫øt c·ª±c ng·∫Øn g·ªçn

V√≠ d·ª•:
"Gi·∫£i th√≠ch ƒë∆∞·ª£c nguy√™n l√Ω v·∫≠n h√†nh c·ªßa CPU (ALU, Control Unit)" ‚Üí "Nguy√™n l√Ω CPU"
"Vi·∫øt v√† g·ª° l·ªói ch∆∞∆°ng tr√¨nh Assembly" ‚Üí "L·∫≠p tr√¨nh Assembly"

VƒÉn b·∫£n: {text[:1500]}

R√∫t g·ªçn:"""
        
        response = self.gemini_client.generate_content(prompt)
        summary = response.text.strip()
        
        logger.info(f"‚úÖ [GEMINI SUCCESS] Summary: {len(text)} ‚Üí {len(summary)} chars")
        return summary
    
    def _summarize_with_local_model(self, text: str, max_length: int) -> str:
        """Summarize using local T5 model"""
        logger.info(f"ü§ñ [LOCAL MODEL] Summarizing text: {len(text)} chars")
        
        # T5 prefix format
        prompt = f"vietnews: {text}"
        
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        # Move to device if needed
        if hasattr(self, 'device') and self.device != "cpu":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # Generate summary
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode - T5 returns clean summary directly
        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        
        logger.info(f"‚úÖ [LOCAL MODEL SUCCESS] Summary: {len(summary)} chars")
        return summary
    
    def _extractive_summarize(self, text: str, max_length: int = 100) -> str:
        """
        Simple extractive summarization - take first N sentences and ensure max_length
        Fallback when AI model not available
        """
        # Split into sentences
        sentences = []
        for delimiter in ['. ', '.\n', '! ', '? ']:
            if delimiter in text:
                parts = [s.strip() for s in text.split(delimiter) if s.strip()]
                sentences = parts
                break
        
        if not sentences:
            # No sentences, just truncate at word boundary
            if len(text) <= max_length:
                return text
            # Truncate at word boundary without "..."
            truncated = text[:max_length].rsplit(' ', 1)[0]
            return truncated.strip()
        
        # Take first sentences that fit within max_length
        result = []
        current_len = 0
        for sent in sentences:
            sent_with_period = sent if sent.endswith('.') else sent + '.'
            # If adding this sentence exceeds max_length
            if current_len + len(sent_with_period) + 1 > max_length:
                # If we already have some sentences, stop here (no "..." needed)
                if result:
                    break
                # If even first sentence is too long, truncate at word boundary
                available = max_length - current_len
                if available > 50:  # Only if we have reasonable space
                    truncated = sent[:available].rsplit(' ', 1)[0]
                    return truncated.strip()  # No "..." - just truncate cleanly
                else:
                    # Just truncate the whole text
                    truncated = text[:max_length].rsplit(' ', 1)[0]
                    return truncated.strip()
                break
            result.append(sent_with_period)
            current_len += len(sent_with_period) + 1
        
        if result:
            summary = ' '.join(result)
            logger.info(f"üìù Extractive summary: {len(text)} -> {len(summary)} chars")
            return summary
        
        # Final fallback - truncate at word boundary (no "...")
        truncated = text[:max_length].rsplit(' ', 1)[0]
        return truncated.strip()

    def _summarize_with_ai(self, syllabus_data: Dict) -> Dict:
        """
        T·∫°o t√≥m t·∫Øt c√≥ c·∫•u tr√∫c t·ª´ d·ªØ li·ªáu ƒë·ªÅ c∆∞∆°ng theo format chu·∫©n
        """
        try:
            # DEBUG: Print all keys received from Java
            logger.info(f"üîç DEBUG - Received syllabus_data keys: {list(syllabus_data.keys())}")
            
            # Extract syllabus information
            course_name = syllabus_data.get('course_name', 'N/A')
            description = syllabus_data.get('description', '')
            learning_outcomes = syllabus_data.get('learning_outcomes', [])
            assessment_scheme = syllabus_data.get('assessment_scheme', [])
            objectives = syllabus_data.get('objectives', [])
            theory_hours = syllabus_data.get('theory_hours', 0)
            practice_hours = syllabus_data.get('practice_hours', 0)
            prerequisites = syllabus_data.get('prerequisites', [])
            textbooks = syllabus_data.get('textbooks', [])
            references = syllabus_data.get('references', [])
            weekly_content = syllabus_data.get('weekly_content', [])
            
            # DEBUG: Print sample data for important fields
            if learning_outcomes:
                logger.info(f"üîç DEBUG - learning_outcomes sample: {learning_outcomes[0] if len(learning_outcomes) > 0 else 'empty'}")
            if assessment_scheme:
                logger.info(f"üîç DEBUG - assessment_scheme sample: {assessment_scheme[0] if len(assessment_scheme) > 0 else 'empty'}")
            
            logger.info(f"üìä Processing syllabus: {course_name}")
            logger.info(f"   Description length: {len(description) if description else 0} chars")
            logger.info(f"   Objectives: {len(objectives) if isinstance(objectives, list) else 'string' if objectives else 0}")
            logger.info(f"   Learning outcomes (CLO): {len(learning_outcomes)} items")
            logger.info(f"   Assessment scheme: {len(assessment_scheme)} items")
            logger.info(f"   Assessment matrix: {len(syllabus_data.get('assessment_matrix', []))} items")
            
            # 1. M√¥ t·∫£ h·ªçc ph·∫ßn - T√ìM T·∫ÆT G·ªåN b·∫±ng AI (80 k√Ω t·ª± t·ªëi ƒëa)
            mo_ta = self._summarize_text(description, max_length=80) if description else "Kh√¥ng c√≥ th√¥ng tin"
            logger.info(f"‚úÖ Description summarized: {len(description) if description else 0} -> {len(mo_ta)} chars")
            
            # 2. M·ª•c ti√™u h·ªçc ph·∫ßn - T√ìM T·∫ÆT T·ª™NG M·ª§C b·∫±ng extractive summarization
            muc_tieu = []
            logger.info(f"üìù Processing objectives: type={type(objectives)}, length={len(objectives) if isinstance(objectives, (list, str)) else 0}")
            if objectives:
                # Check if objectives is a list or string
                if isinstance(objectives, list):
                    for obj in objectives:
                        if isinstance(obj, dict):
                            # If it's a dict, extract text from common keys
                            obj_text = obj.get('text') or obj.get('description') or obj.get('objective') or str(obj)
                        else:
                            obj_text = obj if isinstance(obj, str) else str(obj)
                        if obj_text and obj_text.strip():
                            # T√≥m t·∫Øt m·ªói m·ª•c ti√™u b·∫±ng AI (20 k√Ω t·ª± t·ªëi ƒëa)
                            summarized = self._summarize_text(obj_text.strip(), max_length=20)
                            muc_tieu.append(summarized)
                            logger.debug(f"   Objective: {len(obj_text)} -> {len(summarized)} chars")
                elif isinstance(objectives, str) and objectives.strip():
                    # If it's a string, split by common delimiters or add as single item
                    if '\n' in objectives:
                        parts = [o.strip() for o in objectives.split('\n') if o.strip()]
                        muc_tieu = [self._summarize_text(p, max_length=20) for p in parts[:5]]  # Limit to 5 objectives
                    elif '. ' in objectives and len(objectives) > 50:  # Only split if it's a long text
                        parts = objectives.split('. ')
                        formatted = [o.strip() + ('.' if not o.endswith('.') else '') for o in parts if o.strip()]
                        muc_tieu = [self._summarize_text(f, max_length=20) for f in formatted[:5]]  # Limit to 5
                    else:
                        muc_tieu = [self._summarize_text(objectives.strip(), max_length=50)]
            
            logger.info(f"‚úÖ Objectives processed: {len(muc_tieu)} items")
            
            # 3. Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y - T√ìM T·∫ÆT
            phuong_phap_giang_day = []
            
            # First check if there's a teaching_method field
            teaching_method = syllabus_data.get('teaching_method', '')
            if teaching_method:
                if isinstance(teaching_method, str) and teaching_method.strip():
                    # Split by newline or comma
                    if '\n' in teaching_method:
                        methods_list = [m.strip() for m in teaching_method.split('\n') if m.strip()]
                        # X·ª≠ l√Ω: c·∫Øt n·ªôi dung sau d·∫•u ":", r·ªìi m·ªõi t√≥m t·∫Øt
                        phuong_phap_giang_day = []
                        for m in methods_list:
                            # C·∫Øt n·ªôi dung sau d·∫•u ":"
                            if ':' in m:
                                m = m.split(':')[0].strip()
                            # T√≥m t·∫Øt n·∫øu v·∫´n c√≤n d√†i
                            phuong_phap_giang_day.append(self._summarize_text(m, max_length=35) if len(m) > 35 else m)
                    elif ',' in teaching_method:
                        methods_list = [m.strip() for m in teaching_method.split(',') if m.strip()]
                        phuong_phap_giang_day = []
                        for m in methods_list:
                            if ':' in m:
                                m = m.split(':')[0].strip()
                            phuong_phap_giang_day.append(self._summarize_text(m, max_length=35) if len(m) > 35 else m)
                    else:
                        # C·∫Øt sau d·∫•u ":"
                        if ':' in teaching_method:
                            teaching_method = teaching_method.split(':')[0].strip()
                        phuong_phap_giang_day = [self._summarize_text(teaching_method.strip(), max_length=50)]
                elif isinstance(teaching_method, list):
                    # X·ª≠ l√Ω list: c·∫Øt sau d·∫•u ":" tr∆∞·ªõc khi t√≥m t·∫Øt
                    phuong_phap_giang_day = []
                    for m in teaching_method:
                        m_str = str(m).strip()
                        if ':' in m_str:
                            m_str = m_str.split(':')[0].strip()
                        phuong_phap_giang_day.append(self._summarize_text(m_str, max_length=35) if len(m_str) > 35 else m_str)
            
            # Fallback to weekly_content if teaching_method is empty
            if not phuong_phap_giang_day and weekly_content and isinstance(weekly_content, list):
                for week in weekly_content[:3]:
                    if isinstance(week, dict):
                        activities = week.get('activities', '')
                        if activities and activities not in phuong_phap_giang_day:
                            phuong_phap_giang_day.append(activities)
            
            # Final fallback to default methods
            if not phuong_phap_giang_day:
                phuong_phap_giang_day = ["B√†i gi·∫£ng tr√™n l·ªõp", "Th·∫£o lu·∫≠n nh√≥m", "B√†i t·∫≠p th·ª±c h√†nh"]
            
            # 4. Ph∆∞∆°ng ph√°p ƒë√°nh gi√°
            phuong_phap_danh_gia = []
            if assessment_scheme and len(assessment_scheme) > 0:
                for assess in assessment_scheme:
                    if isinstance(assess, dict):
                        method = assess.get('method', '')
                        weight = assess.get('weight', '')
                        if method:
                            phuong_phap_danh_gia.append({
                                "method": method,
                                "weight": str(weight)
                            })
            
            # 5. Gi√°o tr√¨nh ch√≠nh
            giao_trinh_chinh = []
            if textbooks:
                if isinstance(textbooks, str) and textbooks.strip():
                    # If textbooks is a string, split by newline
                    lines = [line.strip() for line in textbooks.split('\n') if line.strip()]
                    for line in lines[:5]:  # Limit to 5 books
                        giao_trinh_chinh.append({"title": line})
                elif isinstance(textbooks, list) and len(textbooks) > 0:
                    for book in textbooks:
                        if isinstance(book, dict):
                            if book.get('type') == 'required' or not book.get('type'):
                                giao_trinh_chinh.append({
                                    "title": book.get('title', ''),
                                    "authors": book.get('authors', ''),
                                    "year": book.get('year', '')
                                })
                        elif isinstance(book, str) and book.strip():
                            giao_trinh_chinh.append({"title": book.strip()})
            
            # 6. T√†i li·ªáu tham kh·∫£o
            tai_lieu_tham_khao = []
            
            # First check textbooks for reference type
            if textbooks and isinstance(textbooks, list) and len(textbooks) > 0:
                for book in textbooks:
                    if isinstance(book, dict):
                        if book.get('type') == 'reference':
                            tai_lieu_tham_khao.append({
                                "title": book.get('title', ''),
                                "authors": book.get('authors', ''),
                                "year": book.get('year', '')
                            })
            
            # Then process references field
            if references:
                if isinstance(references, str) and references.strip():
                    # If references is a string, split by newline
                    lines = [line.strip() for line in references.split('\n') if line.strip()]
                    for line in lines[:10]:  # Limit to 10 references
                        tai_lieu_tham_khao.append({"title": line})
                elif isinstance(references, list) and len(references) > 0:
                    for ref in references:
                        if isinstance(ref, dict):
                            tai_lieu_tham_khao.append({
                                "title": ref.get('title', ''),
                                "authors": ref.get('authors', ''),
                                "year": ref.get('year', '')
                            })
                        elif isinstance(ref, str) and ref.strip():
                            tai_lieu_tham_khao.append({"title": ref.strip()})
            
            # 7. Nhi·ªám v·ª• c·ªßa Sinh vi√™n - T√ìM T·∫ÆT
            nhiem_vu = []
            student_duties = syllabus_data.get('student_duties', '')
            if student_duties:
                # If data from database exists
                if isinstance(student_duties, str):
                    if '. ' in student_duties:
                        duties_list = [nv.strip() + '.' for nv in student_duties.split('. ') if nv.strip()]
                        # T√≥m t·∫Øt t·ª´ng nhi·ªám v·ª•
                        nhiem_vu = [self._summarize_text(duty, max_length=80) for duty in duties_list]
                    else:
                        nhiem_vu = [self._summarize_text(student_duties, max_length=100)]
                elif isinstance(student_duties, list):
                    # T√≥m t·∫Øt t·ª´ng item trong list
                    nhiem_vu = [self._summarize_text(str(duty), max_length=80) for duty in student_duties if str(duty).strip()]
            
            # If no data from database, generate generic template
            if not nhiem_vu:
                nhiem_vu = [
                    f"Tham gia ƒë·∫ßy ƒë·ªß {theory_hours + practice_hours} ti·∫øt h·ªçc ({theory_hours} l√Ω thuy·∫øt + {practice_hours} th·ª±c h√†nh)",
                    "Ho√†n th√†nh c√°c b√†i t·∫≠p ƒë∆∞·ª£c giao ƒë√∫ng h·∫°n",
                    "Tham gia th·∫£o lu·∫≠n v√† l√†m vi·ªác nh√≥m t√≠ch c·ª±c",
                    "Chu·∫©n b·ªã b√†i tr∆∞·ªõc khi ƒë·∫øn l·ªõp"
                ]
            
            # 8. Chu·∫©n ƒë·∫ßu ra h·ªçc ph·∫ßn (CLO) - T√ìM T·∫ÆT M√î T·∫¢
            clo_list = []
            if learning_outcomes and len(learning_outcomes) > 0:
                for clo in learning_outcomes:
                    if isinstance(clo, dict):
                        desc = clo.get('description', '')
                        # T√≥m t·∫Øt CLO description b·∫±ng AI (40 k√Ω t·ª± t·ªëi ƒëa)
                        summarized_desc = self._summarize_text(desc, max_length=40) if desc else ""
                        clo_list.append({
                            "code": clo.get('code', ''),
                            "description": summarized_desc,
                            "bloom_level": clo.get('bloom_level', ''),
                            "weight": str(clo.get('weight', ''))
                        })
            else:
                logger.warning("‚ö†Ô∏è No CLOs received from backend - check if syllabus was saved properly")
            
            # 9. Ma tr·∫≠n ƒë√°nh gi√° (Assessment Matrix)
            ma_tran_danh_gia = []
            assessment_matrix = syllabus_data.get('assessment_matrix', [])
            if assessment_matrix and isinstance(assessment_matrix, list):
                for item in assessment_matrix:
                    if isinstance(item, dict):
                        ma_tran_danh_gia.append({
                            "method": item.get('method', ''),
                            "form": item.get('form', ''),
                            "criteria": self._summarize_text(item.get('criteria', ''), max_length=80) if item.get('criteria') else '',
                            "weight": str(item.get('weight', ''))
                        })
            else:
                logger.warning(" No assessment matrix received from backend - check if syllabus was saved properly")
            
            result = {
                "course_name": course_name,
                "mo_ta_hoc_phan": mo_ta,
                "muc_tieu_hoc_phan": muc_tieu,
                "phuong_phap_giang_day": phuong_phap_giang_day,
                "phuong_phap_danh_gia": phuong_phap_danh_gia,
                "giao_trinh_chinh": giao_trinh_chinh,
                "tai_lieu_tham_khao": tai_lieu_tham_khao,
                "nhiem_vu_sinh_vien": nhiem_vu,
                "clo": clo_list,
                "ma_tran_danh_gia": ma_tran_danh_gia
            }
            
            logger.info("=" * 80)
            logger.info(" STRUCTURED SUMMARY COMPLETED")
            logger.info(f" Summary stats:")
            logger.info(f"   - Description: {len(mo_ta)} chars")
            logger.info(f"   - Objectives: {len(muc_tieu)} items")
            logger.info(f"   - Teaching methods: {len(phuong_phap_giang_day)} items")
            logger.info(f"   - Assessment methods: {len(phuong_phap_danh_gia)} items")
            logger.info(f"   - CLOs: {len(clo_list)} items")
            logger.info(f"   - Assessment matrix: {len(ma_tran_danh_gia)} items")
            logger.info("=" * 80)
            return result
            
        except Exception as e:
            logger.error(f" Summary creation failed: {e}", exc_info=True)
            # Fallback to basic structured summary
            return self._create_structured_summary(syllabus_data)
    
    def _generate_summary(self, prompt: str, max_length: int = 150) -> str:
        """
        Generate summary using BARTpho
        """
        try:
            # Tokenize (BARTpho doesn't use token_type_ids)
            inputs = self.tokenizer(
                prompt,
                max_length=512,
                truncation=True,
                return_tensors="pt",
                add_special_tokens=True
            )
            # Remove token_type_ids if present (not used by BARTpho)
            if 'token_type_ids' in inputs:
                del inputs['token_type_ids']
            
            inputs = inputs.to(self.device)
            
            # Generate with better parameters for quality
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=max_length,
                    min_length=20,
                    num_beams=5,
                    no_repeat_ngram_size=3,
                    repetition_penalty=2.0,
                    length_penalty=1.0,
                    early_stopping=True
                )
            
            # Decode
            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return summary.strip()
            
        except Exception as e:
            logger.error(f" Generation failed: {e}")
            return prompt[:max_length]  # Fallback to truncated input
    
    def _format_learning_outcomes(self, outcomes: list) -> str:
        """Format learning outcomes for prompt"""
        if not outcomes:
            return "Kh√¥ng c√≥ th√¥ng tin"
        formatted = []
        for o in outcomes[:5]:
            if isinstance(o, dict):
                formatted.append(f"- {o.get('description', str(o))}")
            else:
                formatted.append(f"- {str(o)}")
        return "\n".join(formatted)
    
    def _format_assessment_scheme(self, scheme: list) -> str:
        """Format assessment scheme for prompt"""
        if not scheme:
            return "Kh√¥ng c√≥ th√¥ng tin"
        return "\n".join([f"- {s.get('type', 'N/A')}: {s.get('weight', 0)}%" for s in scheme])
    
    def _extract_highlights(self, syllabus_data: Dict) -> Dict:
        """Extract key highlights from syllabus data"""
        theory_hours = syllabus_data.get('theory_hours', 0)
        practice_hours = syllabus_data.get('practice_hours', 0)
        total_hours = theory_hours + practice_hours
        assessment_scheme = syllabus_data.get('assessment_scheme', [])
        learning_outcomes = syllabus_data.get('learning_outcomes', [])
        
        # Determine difficulty
        difficulty_level = "MEDIUM"
        if total_hours > 60:
            difficulty_level = "HIGH"
        elif total_hours < 30:
            difficulty_level = "EASY"
        
        return {
            "difficulty": {
                "level": difficulty_level,
                "description": f"{difficulty_level.capitalize()} - T·ªïng {total_hours} ti·∫øt"
            },
            "duration": {
                "theory_hours": theory_hours,
                "practice_hours": practice_hours,
                "total_hours": total_hours,
                "description": f"{theory_hours} l√Ω thuy·∫øt + {practice_hours} ti·∫øt th·ª±c h√†nh"
            },
            "assessment": {
                "summary": f"C√≥ {len(assessment_scheme) if assessment_scheme else 0} ph∆∞∆°ng ph√°p ƒë√°nh gi√°",
                "breakdown": assessment_scheme if assessment_scheme else []
            },
            "skills_acquired": {
                "summary": f"C√≥ {len(learning_outcomes) if learning_outcomes else 0} k·∫øt qu·∫£ h·ªçc t·∫≠p",
                "key_skills": [
                    o.get('description', str(o))[:100] if isinstance(o, dict) else str(o)[:100] 
                    for o in (learning_outcomes[:5] if learning_outcomes else [])
                ]
            }
        }
    
    def _generate_recommendations(self, syllabus_data: Dict) -> Dict:
        """Generate study recommendations"""
        prerequisites = syllabus_data.get('prerequisites', [])
        theory_hours = syllabus_data.get('theory_hours', 0)
        practice_hours = syllabus_data.get('practice_hours', 0)
        
        # Calculate study time
        total_hours = theory_hours + practice_hours
        hours_per_week = max(4, int(total_hours / 15 * 1.5))  # Assume 15 weeks
        
        return {
            "prerequisites": {
                "required": prerequisites if prerequisites else ["Kh√¥ng c√≥ y√™u c·∫ßu ti√™n quy·∫øt"],
                "description": "N√™n c√≥ ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ c√°c m√¥n ti√™n quy·∫øt" if prerequisites else "Kh√¥ng y√™u c·∫ßu ti√™n quy·∫øt"
            },
            "preparation": {
                "tips": [
                    "ƒê·ªçc tr∆∞·ªõc syllabus v√† t√†i li·ªáu tham kh·∫£o",
                    f"Chu·∫©n b·ªã {hours_per_week} gi·ªù h·ªçc m·ªói tu·∫ßn",
                    "Tham gia ƒë·∫ßy ƒë·ªß c√°c bu·ªïi th·ª±c h√†nh"
                ],
                "description": "Chu·∫©n b·ªã tr∆∞·ªõc khi h·ªçc"
            },
            "study_time": {
                "hours_per_week": hours_per_week,
                "breakdown": f"{int(hours_per_week * 0.6)} gi·ªù l√†m b√†i t·∫≠p + {int(hours_per_week * 0.4)} gi·ªù ƒë·ªçc t√†i li·ªáu",
                "description": f"D√†nh √≠t nh·∫•t {hours_per_week} gi·ªù/tu·∫ßn"
            }
        }
    
    def _create_structured_summary(self, syllabus_data: Dict) -> Dict:
        """Create structured summary without AI generation (fallback)"""
        course_name = syllabus_data.get('course_name', 'N/A')
        description = syllabus_data.get('description', 'Kh√¥ng c√≥ m√¥ t·∫£')
        
        return {
            "overview": {
                "title": course_name,
                "description": description[:200] if len(description) > 200 else description
            },
            "highlights": self._extract_highlights(syllabus_data),
            "recommendations": self._generate_recommendations(syllabus_data)
        }
    
    def _compare_embeddings_similarity(self, text1: str, text2: str) -> float:
        """
        üöÄ TODO: So s√°nh semantic similarity gi·ªØa 2 texts
        
        Example implementation:
        
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        
        # Get embeddings
        emb1 = self._get_embeddings([text1])[0]
        emb2 = self._get_embeddings([text2])[0]
        
        # Calculate cosine similarity
        similarity = cosine_similarity(
            np.array(emb1).reshape(1, -1),
            np.array(emb2).reshape(1, -1)
        )[0][0]
        
        return float(similarity)
        """
        pass
